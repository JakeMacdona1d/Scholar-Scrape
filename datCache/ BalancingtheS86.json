{"title": "Balancing the Scales of Explainable and Transparent AI Agents within Human-Agent Teams", "author": "Sarvesh Sawant, Rohit Mallick, Camden Brady, Kapil Chalil Madathil, Nathan McNeese, Jeff Bertrand", "URL": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=G1CnZ38AAAAJ&cstart=30&pagesize=10&sortby=pubdate&citation_for_view=G1CnZ38AAAAJ:dQ2og3OwTAUC", "abstract": "OBJECTIVE: >With the progressive nature of Human-Agent Teams becoming more and more useful for high-quality work output, there is a proportional need for bi-directional communication between teammates to increase efficient collaboration. This need is centered around the well-known issue of innate mistrust between humans and artificial intelligence, resulting in sub-optimal work. To combat this, computer scientists and humancomputer interaction researchers alike have presented and refined specific solutions to this issue through different methods of AI interpretability. These different methods include explicit AI explanations as well as implicit manipulations of the AI interface, otherwise known as AI transparency. Individually these solutions hold considerable merit in repairing the relationship of trust between teammates, but also have individual flaws. We posit that the combination of different interpretable mechanisms\u00a0\u2026", "issued": "2023/9"}