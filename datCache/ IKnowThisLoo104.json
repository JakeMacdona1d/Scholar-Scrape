{"title": "I Know This Looks Bad, But I Can Explain: Understanding When AI Should Explain Actions In Human-AI Teams", "author": "Rui Zhang, Christopher Flathmann, Geoff Musick, Beau Schelble, Nathan J McNeese", "URL": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=G1CnZ38AAAAJ&cstart=10&pagesize=10&sortby=pubdate&citation_for_view=G1CnZ38AAAAJ:UHK10RUVsp4C", "abstract": "OBJECTIVE: >Explanation of artificial intelligence (AI) decision-making has become an important research area in human\u2013computer interaction (HCI) and computer-supported teamwork research. While plenty of research has investigated AI explanations with an intent to improve AI transparency and human trust in AI, how AI explanations function in teaming environments remains unclear. Given that a major benefit of AI giving explanations is to increase human trust understanding how AI explanations impact human trust is crucial to effective human-AI teamwork. An online experiment was conducted with 156 participants to explore this question by examining how a teammate\u2019s explanations impact the perceived trust of the teammate and the effectiveness of the team and how these impacts vary based on whether the teammate is a human or an AI. This study shows that explanations facilitate trust in AI teammates when explaining\u00a0\u2026", "issued": "2024/2/5"}